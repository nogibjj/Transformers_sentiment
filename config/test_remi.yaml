---
data:
  seq_len: 128
  num_workers: 0
  batch_size: 2
  vocab_size: 255
  pad_token: 0
    
model:
  max_seq_len: 8192
  dim: 128
  n_layers: 6
  n_heads: 8
  ff_dim: 512
  patch_size: 16
  num_classes: 4

training:
  d_lr: 0.0001
  g_lr: 0.0001
  d_iters: 1
  temperature: 100
  gan_hp: 1
  total_iters: 6000
